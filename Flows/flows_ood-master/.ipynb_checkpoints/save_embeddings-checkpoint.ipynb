{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torchvision import transforms as T\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.hub.load('/scratch/as14770/CV-FinalProject/facebookresearch_WSL-Images_master','resnext101_32x32d_wsl', source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x14a7e19805e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "net.avgpool.register_forward_hook(get_activation('avgpool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TINY_IMAGE_NET_ROOT = \"/scratch/as14770/CV-FinalProject/DATA/imagenet-o\"\n",
    "TRAIN_DIR = os.path.join(TINY_IMAGE_NET_ROOT) \n",
    "# VALID_DIR = os.path.join(TINY_IMAGE_NET_ROOT, 'val')\n",
    "# TEST_DIR = os.path.join(TINY_IMAGE_NET_ROOT, 'test')\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create separate validation subfolders for the validation images based on\n",
    "# # their labels indicated in the val_annotations txt file\n",
    "# val_img_dir = os.path.join(VALID_DIR, 'images')\n",
    "\n",
    "# # Open and read val annotations text file\n",
    "# fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
    "# data = fp.readlines()\n",
    "\n",
    "# # Create dictionary to store img filename (word 0) and corresponding\n",
    "# # label (word 1) for every line in the txt file (as key value pair)\n",
    "# val_img_dict = {}\n",
    "# for line in data:\n",
    "#     words = line.split('\\t')\n",
    "#     val_img_dict[words[0]] = words[1]\n",
    "# fp.close()\n",
    "\n",
    "# # Display first 10 entries of resulting val_img_dict dictionary\n",
    "# {k: val_img_dict[k] for k in list(val_img_dict)[:10]}\n",
    "\n",
    "# # Create subfolders (if not present) for validation images based on label,\n",
    "# # and move images into the respective folders\n",
    "# for img, folder in val_img_dict.items():\n",
    "#     newpath = (os.path.join(val_img_dir, folder))\n",
    "#     if not os.path.exists(newpath):\n",
    "#         os.makedirs(newpath)\n",
    "#     if os.path.exists(os.path.join(val_img_dir, img)):\n",
    "#         os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transform_pretrain = T.Compose([\n",
    "                T.Resize((64, 64)),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor()  # Converting cropped images to tensors\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform = preprocess_transform_pretrain)\n",
    "\n",
    "# validation_dataset = datasets.ImageFolder(val_img_dir, transform = preprocess_transform_pretrain)\n",
    "\n",
    "train_loader_tiny_image_net = DataLoader(train_dataset, batch_size, shuffle=True,\n",
    "                                         num_workers=4, pin_memory=True)\n",
    "\n",
    "# val_loader_tiny_image_net = DataLoader(validation_dataset, batch_size, shuffle=False,\n",
    "#                                          num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2048)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "net = net.cuda()\n",
    "data_with_labels = {}\n",
    "embeddings = []\n",
    "labels = []\n",
    "for data, target in train_loader_tiny_image_net:\n",
    "    data = data.cuda()\n",
    "    output = net(data)\n",
    "    size_of_loaded_batch = data.shape[0]\n",
    "    assert size_of_loaded_batch == target.shape[0]\n",
    "    embedding = activation['avgpool'].reshape((size_of_loaded_batch, 2048)).cpu().numpy()\n",
    "    target = target.reshape((size_of_loaded_batch, 1)).numpy()\n",
    "    assert embedding.shape[0] == target.shape[0]\n",
    "    for emb_entry, target_entry in zip(embedding, target):\n",
    "        embeddings.append(emb_entry)\n",
    "        labels.append(target_entry)\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)\n",
    "data_with_labels[\"embeddings\"] = embeddings\n",
    "data_with_labels[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"imagenet-o.npz\", **data_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2048)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "val_data_with_labels = {}\n",
    "embeddings = []\n",
    "labels = []\n",
    "# with os.scandir(TEST_DIR) as test_dataset:\n",
    "#     for test_file in test_dataset:\n",
    "#         img = Image.open(os.path.join(TEST_DIR, test_file)).cuda()\n",
    "#         output = net(img).cpu().nump\n",
    "        \n",
    "        \n",
    "for data, target in val_loader_tiny_image_net:\n",
    "    data = data.cuda()\n",
    "    output = net(data)\n",
    "    size_of_loaded_batch = data.shape[0]\n",
    "    assert size_of_loaded_batch == target.shape[0]\n",
    "    embedding = activation['avgpool'].reshape((size_of_loaded_batch, 2048)).cpu().numpy()\n",
    "    target = target.reshape((size_of_loaded_batch, 1)).numpy()\n",
    "    assert embedding.shape[0] == target.shape[0]\n",
    "    for emb_entry, target_entry in zip(embedding, target):\n",
    "        embeddings.append(emb_entry)\n",
    "        labels.append(target_entry)\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)\n",
    "val_data_with_labels[\"embeddings\"] = embeddings\n",
    "val_data_with_labels[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"tiny_image_net_val.npz\", **val_data_with_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

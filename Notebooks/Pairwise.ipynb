{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets here return x1, x2, label\n",
    "# label -> 0 if x1 and x2 are from the same class\n",
    "# label -> 1 if x1 and x2 are from different classes\n",
    "\n",
    "# imports from general packages\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Citation:\n",
    "# 1. https://discuss.pytorch.org/t/about-normalization-using-pre-trained-vgg16-networks/23560/6\n",
    "def get_mean_and_std_of_dataset(dataset):\n",
    "    single_img, _ = dataset[0]\n",
    "    assert torch.is_tensor(single_img)\n",
    "    num_channels, dim_1, dim_2 = single_img.shape[0], single_img.shape[1], single_img.shape[2]\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 128, num_workers = 4, shuffle = False)\n",
    "    mean = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "    mean = mean / len(loader.dataset)\n",
    "\n",
    "    var = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        var += ((images - mean.unsqueeze(1)) ** 2).sum([0,2])\n",
    "\n",
    "    std = torch.sqrt(var / (len(loader.dataset) * dim_1 * dim_2))\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "def print_dataset_information(dataset, dataset_name, train, verbose):\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Dataset name:\", dataset_name)\n",
    "        print(\"Is train: \", train)\n",
    "        print(\"Number of elements: \", len(dataset))\n",
    "\n",
    "        img, _ = dataset[0]\n",
    "        print(img.shape)\n",
    "        print()\n",
    "\n",
    "        print(\"Transform: \", dataset.transform)\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_custom_data_transform(dataset_name, mean, std):\n",
    "    \n",
    "    if dataset_name in [\"ImageFolder\"]:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(size = (32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Given dataset name is not supported.\")\n",
    "\n",
    "    return transform\n",
    "\n",
    "def load_dataset_with_custom_data_transform(dataset_name, transform):\n",
    "    if dataset_name == \"ImageFolder\":\n",
    "        dataset = datasets.ImageFolder(root = path, transform = transform)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Dataset is not supported\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_dataset(dataset_name, dataset_path, train, id_dataset_name, id_dataset_path, augment, return_mean = False):\n",
    "    id_train_dataset = load_dataset_with_basic_transform(dataset_name = id_dataset_name, train = True, path = id_dataset_path)\n",
    "    mean, std = get_mean_and_std_of_dataset(dataset = id_train_dataset)\n",
    "\n",
    "    print()\n",
    "    print(\"Mean: \", mean)\n",
    "    print(\"Std: \", std)\n",
    "    print()\n",
    "\n",
    "    custom_transform = get_custom_data_transform(dataset_name = dataset_name, mean = mean, std = std)\n",
    "    dataset = load_dataset_with_custom_data_transform(dataset_name = dataset_name, train = train, path = dataset_path, transform = custom_transform)\n",
    "\n",
    "    print()\n",
    "    print(\"###########################\")\n",
    "    print(\"printing information on the loaded dataset!\")\n",
    "    print(\"###########################\")\n",
    "    print()\n",
    "\n",
    "    print_dataset_information(dataset = dataset, dataset_name = dataset_name, train = train, verbose = True)\n",
    "\n",
    "    if return_mean:\n",
    "        return dataset, mean\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "def create_label_to_index_mapping(dataset):\n",
    "    mapping = defaultdict(list)\n",
    "    for index in range(len(dataset)):\n",
    "        _, label = dataset[index]\n",
    "        if torch.is_tensor(label):\n",
    "            label = label.item()\n",
    "        mapping[label].append(index)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def generate_per_class_sample(dataset_name, dataset_path, save_path, train, sample_size):\n",
    "    dataset = load_dataset(dataset_name = dataset_name, train = train, path = dataset_path, download = True)\n",
    "    mapping = create_label_to_index_mapping(dataset)\n",
    "    labels = [key for key in mapping.keys()]\n",
    "    labels.sort()\n",
    "    print(\"Labels: \", labels)\n",
    "\n",
    "    per_class_samples = []\n",
    "\n",
    "    for label in labels:\n",
    "        indices = np.random.choice(a = mapping[label], size = sample_size, replace = False)\n",
    "        indices = np.sort(a = indices)\n",
    "\n",
    "        sample = [label] + [indices[i] for i in range(indices.shape[0])]\n",
    "        per_class_samples.append(sample)\n",
    "\n",
    "    np.savetxt(save_path, np.array(per_class_samples, dtype = np.int32), delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "def load_per_class_samples(save_path):\n",
    "    assert os.path.isfile(save_path)\n",
    "    class_to_sample_indices_mapping = {}\n",
    "\n",
    "    with open(save_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = line.split(\",\")\n",
    "            label = int(tokens[0])\n",
    "            samples = [int(tokens[i]) for i in range(1, len(tokens))]\n",
    "\n",
    "            class_to_sample_indices_mapping[label] = samples\n",
    "\n",
    "    return class_to_sample_indices_mapping\n",
    "\n",
    "# prepare validation dataset\n",
    "def generate_validation_dataset(dataset_name, dataset_path, save_path, half_validation_dataset_size):\n",
    "    dataset = load_dataset(dataset_name = dataset_name, train = False, path = dataset_path, download = True)\n",
    "    mapping = create_label_to_index_mapping(dataset)\n",
    "    labels = [key for key in mapping.keys()]\n",
    "    labels.sort()\n",
    "    print(\"Labels: \", labels)\n",
    "\n",
    "    validation_dataset = []\n",
    "\n",
    "    for i in range(half_validation_dataset_size * 2):\n",
    "        # same class -> label 0\n",
    "        if i % 2 == 0:\n",
    "            random_label = np.random.choice(labels)\n",
    "            indices = np.random.choice(a = mapping[random_label], size = 2, replace = False)\n",
    "            label = 0\n",
    "            data_point = [indices[0], indices[1], label]\n",
    "\n",
    "        # different class -> label 1\n",
    "        else:\n",
    "            random_labels = np.random.choice(a = labels, size = 2, replace = False)\n",
    "            index_1 = np.random.choice(a = mapping[random_labels[0]])\n",
    "            index_2 = np.random.choice(a = mapping[random_labels[1]])\n",
    "            label = 1\n",
    "            data_point = [index_1, index_2, label]\n",
    "\n",
    "        validation_dataset.append(data_point)\n",
    "\n",
    "    np.savetxt(save_path, np.array(validation_dataset, dtype= np.int32), delimiter=\",\", fmt = \"%d\")\n",
    "\n",
    "def load_validation_dataset(save_path):\n",
    "    assert os.path.isfile(save_path)\n",
    "\n",
    "    validation_dataset = []\n",
    "    with open(save_path, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = line.split(\",\")\n",
    "            assert len(tokens) == 3\n",
    "            data_point = [int(tokens[0]), int(tokens[1]), int(tokens[2])]\n",
    "\n",
    "            validation_dataset.append(data_point)\n",
    "\n",
    "    return validation_dataset\n",
    "\n",
    "def is_valid_class_to_sample_mapping(mapping):\n",
    "    mapped_list_size = -1\n",
    "    for key in mapping:\n",
    "        mapped_list = mapping[key]\n",
    "        if mapped_list_size == -1:\n",
    "            mapped_list_size = len(mapped_list)\n",
    "        elif mapped_list_size != len(mapped_list):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "class PreSavedPerClassSampledDataset(Dataset):\n",
    "    def __init__(self, dataset, sample_indices_path):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.class_to_sample_indices_mapping = load_per_class_samples(save_path = sample_indices_path)\n",
    "        assert is_valid_class_to_sample_mapping(self.class_to_sample_indices_mapping)\n",
    "\n",
    "        self.class_labels = [class_label for class_label in self.class_to_sample_indices_mapping]\n",
    "        assert len(self.class_labels) > 0\n",
    "        self.class_labels.sort()\n",
    "        self.sample_size_per_class = len(self.class_to_sample_indices_mapping[self.class_labels[0]])\n",
    "\n",
    "        img_indices = []\n",
    "        for class_label in self.class_labels:\n",
    "            img_indices = img_indices + self.class_to_sample_indices_mapping[class_label]\n",
    "        self.img_indices = img_indices\n",
    "\n",
    "        self.len = len(self.img_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_index = self.img_indices[index]\n",
    "        return self.dataset[img_index]\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self.class_labels\n",
    "\n",
    "    def get_sample_size_per_class(self):\n",
    "        return self.sample_size_per_class\n",
    "\n",
    "    def get_sample_from_class(self, class_label, sample_index):\n",
    "        assert sample_index >= 0 and sample_index < self.sample_size_per_class\n",
    "\n",
    "        sample_datapoint_indices = self.class_to_sample_indices_mapping[class_label]\n",
    "        img_index = sample_datapoint_indices[sample_index]\n",
    "\n",
    "        return self.dataset[img_index]\n",
    "\n",
    "# pairwise dataset, with random pairings made over the given dataset\n",
    "# label = 0, both data points from the pair are from the same class\n",
    "# label = 1, datapoints from the pair are from different classes\n",
    "class PairwiseDatasetRandom(Dataset):\n",
    "    def __init__(self, dataset, epoch_length = 10000):\n",
    "        self.dataset = dataset\n",
    "        self.mapping = create_label_to_index_mapping(self.dataset)\n",
    "        self.labels = [key for key in self.mapping.keys()]\n",
    "        self.len = epoch_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1 = None\n",
    "        img2 = None\n",
    "        label = None\n",
    "\n",
    "        # same class -> label = 0\n",
    "        if index % 2 == 0:\n",
    "            random_label = np.random.choice(self.labels)\n",
    "            indices = np.random.choice(a = self.mapping[random_label], size = 2, replace = False)\n",
    "            img1, _ = self.dataset[indices[0]]\n",
    "            img2, _ = self.dataset[indices[1]]\n",
    "\n",
    "            label = 0\n",
    "\n",
    "        # different class -> label = 1\n",
    "        else:\n",
    "            random_labels = np.random.choice(a = self.labels, size = 2, replace = False)\n",
    "            index_1 = np.random.choice(a = self.mapping[random_labels[0]])\n",
    "            index_2 = np.random.choice(a = self.mapping[random_labels[1]])\n",
    "            img1, _ = self.dataset[index_1]\n",
    "            img2, _ = self.dataset[index_2]\n",
    "\n",
    "            label = 1\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "# pairwise validation dataset\n",
    "class PairwiseDatasetPreSaved(Dataset):\n",
    "    def __init__(self, dataset, combination_path):\n",
    "        self.dataset = dataset\n",
    "        self.combinations = load_validation_dataset(save_path = combination_path)\n",
    "        self.len = len(self.combinations)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_point = self.combinations[index]\n",
    "        index_1, index_2, label = data_point[0], data_point[1], data_point[2]\n",
    "\n",
    "        img1, _ = self.dataset[index_1]\n",
    "        img2, _ = self.dataset[index_2]\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "\n",
    "# dataset for classification task (not pairwise task)\n",
    "# that contains selected classes\n",
    "class SelectiveClassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, labels_path):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.partial_labels = []\n",
    "        f = open(labels_path, 'r')\n",
    "        for line in f:\n",
    "            self.partial_labels.append(int(line))\n",
    "        f.close()\n",
    "\n",
    "        self.subset_indices = []\n",
    "        for index in range(len(self.dataset)):\n",
    "            _, label = self.dataset[index]\n",
    "            if label in self.partial_labels:\n",
    "                self.subset_indices.append(index)\n",
    "\n",
    "        self.len = len(self.subset_indices)\n",
    "\n",
    "        self.remapped_labels = {}\n",
    "        for i in range(len(self.partial_labels)):\n",
    "            label = self.partial_labels[i]\n",
    "            self.remapped_labels[label] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[self.subset_indices[index]]\n",
    "        if torch.is_tensor(label):\n",
    "            label = label.item()\n",
    "\n",
    "        new_label = self.remapped_labels[label]\n",
    "        return img, new_label\n",
    "\n",
    "# dataset that contains randomly sampled datapoints of the original dataset\n",
    "class RandomlySampledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, base_rate, choose_randomly = True, fixed_sample_size = None):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        assert base_rate > 0.0 and base_rate <= 1.0\n",
    "        self.base_rate = base_rate\n",
    "\n",
    "        label_to_index_mapping = create_label_to_index_mapping(dataset = self.dataset)\n",
    "        labels = [int(key) for key in label_to_index_mapping.keys()]\n",
    "        labels.sort()\n",
    "\n",
    "        print(\"Labels: \", labels)\n",
    "\n",
    "        sample_indices = []\n",
    "        for label in labels:\n",
    "            indices = label_to_index_mapping[label]\n",
    "\n",
    "            if fixed_sample_size is not None:\n",
    "                sample_size = fixed_sample_size\n",
    "            else:\n",
    "                sample_size = int(len(indices) * self.base_rate)\n",
    "\n",
    "            if choose_randomly:\n",
    "                class_sample = np.random.choice(a = indices, size = sample_size, replace = False)\n",
    "                class_sample = class_sample.tolist()\n",
    "            else:\n",
    "                class_sample = indices[0 : sample_size]\n",
    "\n",
    "            sample_indices = sample_indices + class_sample\n",
    "\n",
    "        self.sample_indices = sample_indices\n",
    "        self.len = len(self.sample_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        new_index = self.sample_indices[index]\n",
    "        return self.dataset[new_index]\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    label_to_index_mapping = create_label_to_index_mapping(dataset = dataset)\n",
    "    labels = [key for key in label_to_index_mapping.keys()]\n",
    "    labels.sort()\n",
    "\n",
    "    print()\n",
    "    print(\"Printing classes and number of element in each class.\")\n",
    "    print()\n",
    "\n",
    "    for label in labels:\n",
    "        print(\"Class: \", label, \" Num datapoints: \", len(label_to_index_mapping[label]))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation:\n",
    "# 1. https://thenewstack.io/tutorial-train-a-deep-learning-model-in-pytorch-and-export-it-to-onnx/\n",
    "# 2. https://discuss.pytorch.org/t/creating-custom-dataset-from-inbuilt-pytorch-datasets-along-with-data-transformations/58270/2\n",
    "# 3. https://medium.com/@sergioalves94/deep-learning-in-pytorch-with-cifar-10-dataset-858b504a6b54\n",
    "# 4. https://github.com/fangpin/siamese-pytorch/blob/master/train.py\n",
    "# 5. https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "def print_message(message, verbose = True):\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "        print(message)\n",
    "        print(\"\")\n",
    "\n",
    "# should_use_scheduler -> True, use scheduler at every batch (instead of every epoch) like Outlier-exposure paper\n",
    "# should_use_scheduler -> False, use scheduler at every epoch\n",
    "def train_model(model, train_loader, optimizer, scheduler, should_use_scheduler):\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "    model.train()\n",
    "\n",
    "    for idx, (img1, img2, label) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            img1 = img1.cuda()\n",
    "            img2 = img2.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model.forward(img1, img2).squeeze()\n",
    "        loss = loss_fn(output, label.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if should_use_scheduler and scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "\n",
    "    total_loss = 0\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (img1, img2, label) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                img1 = img1.cuda()\n",
    "                img2 = img2.cuda()\n",
    "                label = label.cuda()\n",
    "\n",
    "            output = model.forward(img1, img2).squeeze()\n",
    "            loss = loss_fn(output, label.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = (output > 0).long()\n",
    "            num_correct += (pred.squeeze() == label.squeeze()).float().sum().item()\n",
    "            num_total += pred.shape[0]\n",
    "\n",
    "    accuracy = float(num_correct) / num_total\n",
    "    average_loss = float(total_loss) / num_total\n",
    "    return average_loss, accuracy\n",
    "\n",
    "class PairwiseModelTrainer:\n",
    "    def __init__(self, model, model_name, train_loader, validation_loader, optimizer, scheduler = None):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        self.max_val_accuracy = None\n",
    "        self.best_epoch = None\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.train_accuracy_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.val_accuracy_history = []\n",
    "        self.time_history = []\n",
    "\n",
    "    def run_training(self, num_epochs, model_path = None, verbose = True, should_use_scheduler = False):\n",
    "        print_message(message = \"Model training is starting...\", verbose = verbose)\n",
    "\n",
    "        best_model_params = None\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # run the training step\n",
    "            train_model(model = self.model, train_loader = self.train_loader, optimizer = self.optimizer, scheduler = self.scheduler, should_use_scheduler = should_use_scheduler)\n",
    "\n",
    "            # calculate the training and validation loss and accuracy\n",
    "            train_loss, train_accuracy = test_model(model = self.model, test_loader = self.train_loader)\n",
    "            val_loss, val_accuracy = test_model(model = self.model, test_loader = self.validation_loader)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            if not should_use_scheduler and self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            self.train_loss_history.append(train_loss)\n",
    "            self.train_accuracy_history.append(train_accuracy)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            time_per_epoch = end_time - start_time\n",
    "            self.time_history.append(time_per_epoch)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Epoch: \", epoch, \"/\", num_epochs, \" Train Accuracy: \", train_accuracy, \" Val Accuracy: \", val_accuracy)\n",
    "                print(\"            Train loss: \", train_loss, \" Val loss: \", val_loss)\n",
    "                print(\"Learning rate: \", self.scheduler.get_last_lr()[0])\n",
    "                print()\n",
    "\n",
    "            if epoch == 1 or val_accuracy > self.max_val_accuracy:\n",
    "                self.best_epoch = epoch\n",
    "                self.max_val_accuracy = val_accuracy\n",
    "                best_model_params = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        if model_path is not None:\n",
    "            # torch.save(best_model_params, model_path)\n",
    "            torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        if verbose:\n",
    "            total_time = 0\n",
    "            for time_epoch in self.time_history:\n",
    "                total_time += time_epoch\n",
    "\n",
    "            print()\n",
    "            print(\"Total training time: \", total_time, \" seconds\")\n",
    "            print()\n",
    "\n",
    "    def report_peak_performance(self):\n",
    "        if self.max_val_accuracy == None:\n",
    "            print(\"Model has not been trained yet.\")\n",
    "        else:\n",
    "            print()\n",
    "            print(\"Model peaked in validation accuracy at epoch \", self.best_epoch)\n",
    "            print(\"Model peak validation accuracy: \", self.max_val_accuracy)\n",
    "            print()\n",
    "\n",
    "    def save_log(self, log_dir):\n",
    "        print_message(\"Log directory: \" + log_dir, verbose = 1)\n",
    "\n",
    "        record_list = [self.train_loss_history, self.val_loss_history, self.train_accuracy_history, self.val_accuracy_history]\n",
    "        filename_list = [\"train_loss\", \"val_loss\", \"acc\", \"val_acc\"]\n",
    "        filename_prefix = \"_per_epoch.txt\"\n",
    "\n",
    "        for i in range(len(record_list)):\n",
    "            numpy_record = np.array(record_list[i])\n",
    "            filename = filename_list[i] + filename_prefix\n",
    "            filepath = os.path.join(log_dir, filename)\n",
    "            np.savetxt(filepath, numpy_record, delimiter=\",\", fmt = \"%1.4e\")\n",
    "\n",
    "        timer_log = np.array(self.time_history)\n",
    "        total_time = np.sum(timer_log) / 3600.0\n",
    "\n",
    "        appended =  [total_time] + self.time_history\n",
    "        appended_log = np.array(appended)\n",
    "        timer_path = os.path.join(log_dir, \"training_time.txt\")\n",
    "        np.savetxt(timer_path, appended_log, delimiter=\",\", fmt = \"%1.4e\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# import from our scripts\n",
    "from utils.pytorch_pairwise_dataset import load_dataset\n",
    "from utils.pytorch_pairwise_dataset import RandomlySampledDataset\n",
    "from utils.pytorch_pairwise_dataset import SelectiveClassDataset\n",
    "from utils.pytorch_pairwise_dataset import check_dataset\n",
    "\n",
    "from utils.wide_resnet_pytorch import create_wide_resnet\n",
    "from utils.resnet_pytorch import ResNet18, ResNet34, ResNet50\n",
    "\n",
    "from utils.pytorch_classifier_trainer import ClassifierTrainer\n",
    "\n",
    "from utils.siamese_network import process_shared_model_name_wide_resnet\n",
    "\n",
    "from utils.plotting_log_utils import plot_loss\n",
    "from utils.plotting_log_utils import plot_accuracy\n",
    "\n",
    "from utils.get_readable_timestamp import get_readable_timestamp\n",
    "\n",
    "def parse_arguments():\n",
    "    ap = argparse.ArgumentParser()\n",
    "\n",
    "    # dataset and model name and path arguments\n",
    "    ap.add_argument(\"-dataset_name\", \"--dataset_name\", type = str, default = \"CIFAR10\")\n",
    "    ap.add_argument(\"-dataset_path\", \"--dataset_path\", type = str)\n",
    "    ap.add_argument(\"-num_classes\", \"--num_classes\", type = int, default = 10)\n",
    "    ap.add_argument(\"-model_type\", \"--model_type\", type = str, default = \"ResNet34\")\n",
    "\n",
    "    # training arguments\n",
    "    ap.add_argument(\"-train_batch_size\", \"--train_batch_size\", type = int, default = 32)\n",
    "    ap.add_argument(\"-val_batch_size\", \"--val_batch_size\", type = int, default = 1024)\n",
    "    ap.add_argument(\"-num_workers\", \"--num_workers\", type = int, default = 4)\n",
    "\n",
    "    ap.add_argument(\"-lr\", \"--lr\", type = float, default = 0.1)\n",
    "    ap.add_argument(\"-momentum\", \"--momentum\", type = float, default = 0.9)\n",
    "    ap.add_argument(\"-weight_decay\", \"--weight_decay\", type = float, default = 0.0005)\n",
    "\n",
    "    ap.add_argument(\"-num_epochs\", \"--num_epochs\", type = int, default = 200)\n",
    "    ap.add_argument(\"-use_nesterov\", \"--use_nesterov\", type = int, default = 1)\n",
    "    ap.add_argument(\"-verbose\", \"--verbose\", type = bool, default = True)\n",
    "    ap.add_argument(\"-use_default_scheduler\", \"--use_default_scheduler\", type = int, default = 0, choices = [0, 1])\n",
    "    ap.add_argument(\"-should_plot\", \"--should_plot\", type = int, default = 0)\n",
    "\n",
    "    # saving directory arguments\n",
    "    ap.add_argument(\"-model_name\", \"--model_name\", type = str, default = \"MSP model\")\n",
    "    ap.add_argument(\"-model_save_path\", \"--model_save_path\", type = str)\n",
    "    ap.add_argument(\"-plot_directory\", \"--plot_directory\", type = str, default = \"./\")\n",
    "    ap.add_argument(\"-log_directory\", \"--log_directory\", type = str, default = \"./\")\n",
    "\n",
    "    # training on partial datasets\n",
    "    ap.add_argument(\"-base_rate\", \"--base_rate\", type = float, default = 1.0)\n",
    "    ap.add_argument(\"-use_partial_dataset\", \"--use_partial_dataset\", type = int, default = 0, choices = [0, 1])\n",
    "    ap.add_argument(\"-partial_dataset_path_prefix\", \"--partial_dataset_path_prefix\", type = str, default = \"./\")\n",
    "    ap.add_argument(\"-partial_dataset_filename\", \"--partial_dataset_filename\", type = str, default = \"dataset_1/partial_dataset_labels.txt\")\n",
    "\n",
    "    script_arguments = vars(ap.parse_args())\n",
    "    return script_arguments\n",
    "\n",
    "def print_arguments(args):\n",
    "    print()\n",
    "    print(\"Arguments given for the script...\")\n",
    "    for key in args:\n",
    "        print(\"Key: \", key, \" Value: \", args[key])\n",
    "    print()\n",
    "\n",
    "def create_dataloaders(args):\n",
    "    train_dataset = load_dataset(dataset_name = args[\"dataset_name\"],\n",
    "                                 dataset_path = args[\"dataset_path\"],\n",
    "                                 train = True,\n",
    "                                 id_dataset_name = args[\"dataset_name\"],\n",
    "                                 id_dataset_path = args[\"dataset_path\"],\n",
    "                                 augment = True)\n",
    "\n",
    "    if args[\"base_rate\"] < 1.0:\n",
    "        train_dataset = RandomlySampledDataset(dataset = train_dataset, base_rate = args[\"base_rate\"], choose_randomly = False)\n",
    "\n",
    "    validation_dataset = load_dataset(dataset_name = args[\"dataset_name\"],\n",
    "                                      dataset_path = args[\"dataset_path\"],\n",
    "                                      train = False,\n",
    "                                      id_dataset_name = args[\"dataset_name\"],\n",
    "                                      id_dataset_path = args[\"dataset_path\"],\n",
    "                                      augment = False)\n",
    "\n",
    "    if args[\"use_partial_dataset\"] == 1:\n",
    "        labels_path = args[\"partial_dataset_path_prefix\"] + args[\"partial_dataset_filename\"]\n",
    "        train_dataset = SelectiveClassDataset(dataset = train_dataset, labels_path = labels_path)\n",
    "        validation_dataset = SelectiveClassDataset(dataset = validation_dataset, labels_path = labels_path)\n",
    "\n",
    "    check_dataset(dataset = train_dataset)\n",
    "    check_dataset(dataset = validation_dataset)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                   batch_size = args[\"train_batch_size\"],\n",
    "                                                   shuffle = True,\n",
    "                                                   num_workers = args[\"num_workers\"])\n",
    "\n",
    "    validation_dataloader = torch.utils.data.DataLoader(dataset = validation_dataset,\n",
    "                                                        batch_size = args[\"val_batch_size\"],\n",
    "                                                        shuffle = False,\n",
    "                                                        num_workers = args[\"num_workers\"])\n",
    "\n",
    "    return train_dataloader, validation_dataloader\n",
    "\n",
    "def create_resnet_given_params(model_name, num_classes, num_input_channels):\n",
    "    resnet_model = None\n",
    "\n",
    "    if model_name == \"ResNet18\":\n",
    "        resnet_model = ResNet18(contains_last_layer = True, num_input_channels = num_input_channels, num_classes = num_classes)\n",
    "\n",
    "    elif model_name == \"ResNet34\":\n",
    "        resnet_model = ResNet34(contains_last_layer = True, num_input_channels = num_input_channels, num_classes = num_classes)\n",
    "\n",
    "    elif model_name == \"ResNet50\":\n",
    "        resnet_model = ResNet50(contains_last_layer = True, num_input_channels = num_input_channels, num_classes = num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Model name not supported')\n",
    "\n",
    "    return resnet_model\n",
    "\n",
    "def create_wide_resnet_model(args):\n",
    "    architecture_map = process_shared_model_name_wide_resnet(shared_model_name = args[\"model_type\"])\n",
    "    dataset_name = args[\"dataset_name\"]\n",
    "\n",
    "    kwargs = {\"depth\": architecture_map[\"depth\"],\n",
    "              \"widen_factor\": architecture_map[\"widen_factor\"],\n",
    "              \"dropRate\": architecture_map[\"dropRate\"],\n",
    "              \"num_classes\": args[\"num_classes\"],\n",
    "              \"contains_last_layer\": True}\n",
    "\n",
    "    if dataset_name == \"CIFAR10\" or dataset_name == \"CIFAR100\" or dataset_name == \"CIFAR\" or dataset_name == \"SVHN\" or dataset_name == \"CIFAR100Coarse\":\n",
    "        kwargs[\"num_input_channels\"] = 3\n",
    "        model = create_wide_resnet(**kwargs)\n",
    "\n",
    "    elif dataset_name == \"MNIST\":\n",
    "        kwargs[\"num_input_channels\"] = 1\n",
    "        model = create_wide_resnet(**kwargs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Dataset name not supported\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_resnet_model(args):\n",
    "    dataset_name = args[\"dataset_name\"]\n",
    "    kwargs = {\"model_name\": args[\"model_type\"],\n",
    "              \"num_classes\": args[\"num_classes\"]}\n",
    "\n",
    "    if dataset_name == \"CIFAR10\" or dataset_name == \"CIFAR100\" or dataset_name == \"CIFAR\" or dataset_name == \"SVHN\" or dataset_name == \"CIFAR100Coarse\":\n",
    "        kwargs[\"num_input_channels\"] = 3\n",
    "        model = create_resnet_given_params(**kwargs)\n",
    "\n",
    "    elif dataset_name == \"MNIST\":\n",
    "        kwargs[\"num_input_channels\"] = 1\n",
    "        model = create_resnet_given_params(**kwargs)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Dataset name not supported\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(args):\n",
    "    if args[\"model_type\"].find(\"WideResNet\") == 0:\n",
    "        model = create_wide_resnet_model(args)\n",
    "\n",
    "    elif args[\"model_type\"].find(\"ResNet\") >= 0:\n",
    "        model = create_resnet_model(args)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Given model type is not supported\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_optimizer_and_scheduler(args, model, len_train_dataloader):\n",
    "    use_nesterov = None\n",
    "    if args[\"use_nesterov\"] == 1:\n",
    "        use_nesterov = True\n",
    "    elif args[\"use_nesterov\"] == 0:\n",
    "        use_nesterov = False\n",
    "    else:\n",
    "        raise ValueError(\"argument for using nesterov momentum, is not supported\")\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr = args[\"lr\"],\n",
    "                                momentum = args[\"momentum\"],\n",
    "                                nesterov = use_nesterov,\n",
    "                                weight_decay = args[\"weight_decay\"])\n",
    "\n",
    "    if args[\"use_default_scheduler\"] == 1:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = args[\"num_epochs\"])\n",
    "        \n",
    "    else:\n",
    "        def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "            return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "        # scheduler from outlier exposure\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda = lambda step: cosine_annealing(step,\n",
    "                                                                                                          args[\"num_epochs\"] * len_train_dataloader,\n",
    "                                                                                                          1,  # since lr_lambda computes multiplicative factor\n",
    "                                                                                                          1e-6 / args[\"lr\"]))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def run_model_training(args):\n",
    "    model = create_model(args = args)\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"Model created.\")\n",
    "        print(model)\n",
    "\n",
    "    train_dataloader, val_dataloader = create_dataloaders(args = args)\n",
    "    if args[\"verbose\"]:\n",
    "        print()\n",
    "        print(\"Train dataloader and validation dataloader created\")\n",
    "        print()\n",
    "\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(args = args, model = model, len_train_dataloader = len(train_dataloader))\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"Optimizer and scheduler created\")\n",
    "        print(\"Optimizer: \", optimizer)\n",
    "        print(\"scheduler: \", scheduler)\n",
    "        print(\"\")\n",
    "\n",
    "    readable_timestamp = get_readable_timestamp() + \"_\" + str(np.random.randint(1000000))\n",
    "    model_name = args[\"model_name\"]\n",
    "    model_path = os.path.join(args[\"model_save_path\"], readable_timestamp)\n",
    "    if not os.path.isdir(args[\"model_save_path\"]):\n",
    "        os.makedirs(args[\"model_save_path\"])\n",
    "\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"Model name: \", model_name)\n",
    "        print(\"model path: \", model_path)\n",
    "        print(\"\")\n",
    "\n",
    "    model_trainer = ClassifierTrainer(model = model, model_name = model_name,\n",
    "                                      train_loader = train_dataloader, validation_loader = val_dataloader,\n",
    "                                      optimizer = optimizer, scheduler = scheduler)\n",
    "\n",
    "    # should_use_scheduler -> True, use scheduler at every batch (instead of every epoch) like Outlier-exposure paper\n",
    "    # should_use_scheduler -> False, use scheduler at every epoch\n",
    "    if args[\"use_default_scheduler\"] == 1:\n",
    "        should_use_scheduler = False\n",
    "    else:\n",
    "        should_use_scheduler = True\n",
    "\n",
    "    model_trainer.run_training(num_epochs = args[\"num_epochs\"],\n",
    "                               model_path = model_path,\n",
    "                               verbose = args[\"verbose\"],\n",
    "                               should_use_scheduler = should_use_scheduler)\n",
    "\n",
    "    model_trainer.report_peak_performance()\n",
    "\n",
    "    plot_directory = os.path.join(args[\"plot_directory\"], readable_timestamp)\n",
    "    if not os.path.isdir(plot_directory):\n",
    "        os.makedirs(plot_directory)\n",
    "\n",
    "    if args[\"should_plot\"] > 0:\n",
    "        if args[\"verbose\"]:\n",
    "            print(\"Plot directory: \", plot_directory)\n",
    "            print(\"Plotting loss and accuracy...\")\n",
    "            print()\n",
    "\n",
    "        plot_loss(model_name, model_trainer.train_loss_history, model_trainer.val_loss_history, plot_directory)\n",
    "        plot_accuracy(model_name, model_trainer.train_accuracy_history, model_trainer.val_accuracy_history, plot_directory)\n",
    "\n",
    "    log_directory = os.path.join(args[\"log_directory\"], readable_timestamp)\n",
    "    if not os.path.isdir(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"Log directory: \", log_directory)\n",
    "        print(\"Saving training log...\")\n",
    "        print()\n",
    "\n",
    "    model_trainer.save_log(log_directory)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    script_arguments = parse_arguments()\n",
    "    print_arguments(args = script_arguments)\n",
    "    run_model_training(args = script_arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-category",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-rouge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-combination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-palmer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

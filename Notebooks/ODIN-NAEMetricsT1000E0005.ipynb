{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tF5JxCtzS2Z2"
   },
   "outputs": [],
   "source": [
    "# import calibration_tools\n",
    "import os\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.transforms.functional as trnF\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import models\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy import misc\n",
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vs2iAHWh90-V"
   },
   "outputs": [],
   "source": [
    "ROOT = \"/scratch/as14770/CV-FinalProject\"\n",
    "\n",
    "PATH_CONFIDENCE_BASE_IN = ROOT + \"/confidence_Base_In.txt\"\n",
    "PATH_CONFIDENCE_BASE_OUT = ROOT + \"/confidence_Base_Out.txt\"\n",
    "PATH_CONFIDENCE_OUR_IN = ROOT + \"/confidence_Our_In.txt\"\n",
    "PATH_CONFIDENCE_OUR_OUT = ROOT + \"/confidence_Our_Out.txt\"\n",
    "\n",
    "\n",
    "epsilon = 0 # perturbation 21 evenly spaced numbers b/w 0 and 0.004 \n",
    "temperature = 1000  # temperature scaling [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "CUDA_DEVICE = 0   # set to 1 to use gpu\n",
    "# TODO: start and end also may need to be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "81azYT0mGadg"
   },
   "outputs": [],
   "source": [
    "# def tpr95(name):\n",
    "#     #calculate the falsepositive error when tpr is 95%\n",
    "#     # calculate baseline\n",
    "# #     T = 1\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 1 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "#     start = 0.001\n",
    "#     end = 1 \n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     total = 0.0\n",
    "#     fpr = 0.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "#         if tpr <= 0.9505 and tpr >= 0.9495:\n",
    "#             fpr += error2\n",
    "#             total += 1\n",
    "#     fprBase = fpr/total\n",
    "\n",
    "#     # calculate our algorithm\n",
    "# #     T = 1000\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 0.12 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "# #       start = 0.01\n",
    "# #       end = 0.0104  \n",
    "#     start = 0.001\n",
    "#     end = 0.0011\n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     total = 0.0\n",
    "#     fpr = 0.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "#         if tpr <= 0.9505 and tpr >= 0.9495:\n",
    "#             fpr += error2\n",
    "#             total += 1\n",
    "#     fprNew = fpr/total\n",
    "            \n",
    "#     return fprBase, fprNew\n",
    "\n",
    "# def auroc(name):\n",
    "#     #calculate the AUROC\n",
    "#     # calculate baseline\n",
    "# #     T = 1\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 1 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "#     start = 0.001\n",
    "#     end = 1 \n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = indistr[:, 2]\n",
    "#     X1 = outdistr[:, 2]\n",
    "#     aurocBase = 0.0\n",
    "#     fprTemp = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         fpr = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "#         aurocBase += (-fpr+fprTemp)*tpr\n",
    "#         fprTemp = fpr\n",
    "#     aurocBase += fpr * tpr\n",
    "#     # calculate our algorithm\n",
    "# #     T = 1000\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 0.12 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "# #       start = 0.01\n",
    "# #       end = 0.0104   \n",
    "#     start = 0.001\n",
    "#     end = 0.0011\n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     aurocNew = 0.0\n",
    "#     fprTemp = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         fpr = np.sum(np.sum(Y1 >= delta)) / np.float(len(Y1))\n",
    "#         aurocNew += (-fpr+fprTemp)*tpr\n",
    "#         fprTemp = fpr\n",
    "#     aurocNew += fpr * tpr\n",
    "#     return aurocBase, aurocNew\n",
    "\n",
    "# def auprIn(name):\n",
    "#     #calculate the AUPR\n",
    "#     # calculate baseline\n",
    "# #     T = 1\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 1 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "#     start = 0.001\n",
    "#     end = 1   \n",
    "#     gap = (end- start)/100000\n",
    "#     precisionVec = []\n",
    "#     recallVec = []\n",
    "#         #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     auprBase = 0.0\n",
    "#     recallTemp = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tp = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         fp = np.sum(np.sum(Y1 >= delta)) / np.float(len(Y1))\n",
    "#         if tp + fp == 0: continue\n",
    "#         precision = tp / (tp + fp)\n",
    "#         recall = tp\n",
    "#         precisionVec.append(precision)\n",
    "#         recallVec.append(recall)\n",
    "#         auprBase += (recallTemp-recall)*precision\n",
    "#         recallTemp = recall\n",
    "#     auprBase += recall * precision\n",
    "#     #print(recall, precision)\n",
    "\n",
    "#     # calculate our algorithm\n",
    "# #     T = 1000\n",
    "#     inddistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 0.12 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "# #       start = 0.01\n",
    "# #       end = 0.0104   \n",
    "#     start = 0.001\n",
    "#     end = 0.0011\n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     auprNew = 0.0\n",
    "#     recallTemp = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tp = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
    "#         fp = np.sum(np.sum(Y1 >= delta)) / np.float(len(Y1))\n",
    "#         if tp + fp == 0: continue\n",
    "#         precision = tp / (tp + fp)\n",
    "#         recall = tp\n",
    "#         #precisionVec.append(precision)\n",
    "#         #recallVec.append(recall)\n",
    "#         auprNew += (recallTemp-recall)*precision\n",
    "#         recallTemp = recall\n",
    "#     auprNew += recall * precision\n",
    "#     return auprBase, auprNew\n",
    "\n",
    "# def auprOut(name):\n",
    "#     #calculate the AUPR\n",
    "#     # calculate baseline\n",
    "# #     T = 1\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 1 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "#     start = 0.001\n",
    "#     end = 1   \n",
    "#     gap = (end- start)/100000\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     auprBase = 0.0\n",
    "#     recallTemp = 1.0\n",
    "#     for delta in np.arange(end, start, -gap):\n",
    "#         fp = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "#         tp = np.sum(np.sum(Y1 < delta)) / np.float(len(Y1))\n",
    "#         if tp + fp == 0: break\n",
    "#         precision = tp / (tp + fp)\n",
    "#         recall = tp\n",
    "#         auprBase += (recallTemp-recall)*precision\n",
    "#         recallTemp = recall\n",
    "#     auprBase += recall * precision\n",
    "        \n",
    "    \n",
    "#     # calculate our algorithm\n",
    "# #     T = 1000\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 0.12 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "# #       start = 0.01\n",
    "# #       end = 0.0104\n",
    "#     start = 0.001\n",
    "#     end = 0.0011\n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     auprNew = 0.0\n",
    "#     recallTemp = 1.0\n",
    "#     for delta in np.arange(end, start, -gap):\n",
    "#         fp = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "#         tp = np.sum(np.sum(Y1 < delta)) / np.float(len(Y1))\n",
    "#         if tp + fp == 0: break\n",
    "#         precision = tp / (tp + fp)\n",
    "#         recall = tp\n",
    "#         auprNew += (recallTemp-recall)*precision\n",
    "#         recallTemp = recall\n",
    "#     auprNew += recall * precision\n",
    "#     return auprBase, auprNew\n",
    "\n",
    "\n",
    "\n",
    "# def detection(name):\n",
    "#     #calculate the minimum detection error\n",
    "#     # calculate baseline\n",
    "# #     T = 1\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 1 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "#     start = 0.001\n",
    "#     end = 1    \n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     errorBase = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "#         error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "#         errorBase = np.minimum(errorBase, (tpr+error2)/2.0)\n",
    "\n",
    "#     # calculate our algorithm\n",
    "# #     T = 1000\n",
    "#     indistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "#     outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "# #     if name == \"CIFAR-10\": \n",
    "# #       start = 0.1\n",
    "# #       end = 0.12 \n",
    "# #     if name == \"CIFAR-100\": \n",
    "# #       start = 0.01\n",
    "# #       end = 0.0104 \n",
    "#     start = 0.001\n",
    "#     end = 0.0011\n",
    "#     gap = (end- start)/100000\n",
    "#     #f = open(\"./{}/{}/T_{}.txt\".format(nnName, dataName, T), 'w')\n",
    "#     Y1 = outdistr[:, 2]\n",
    "#     X1 = indistr[:, 2]\n",
    "#     errorNew = 1.0\n",
    "#     for delta in np.arange(start, end, gap):\n",
    "#         tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "#         error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "#         errorNew = np.minimum(errorNew, (tpr+error2)/2.0)\n",
    "            \n",
    "#     return errorBase, errorNew\n",
    "\n",
    "\n",
    "# def metric(nn):\n",
    "#     indis = \"Imagenet-1k\"\n",
    "#     fprBase, fprNew = tpr95(indis)\n",
    "#     errorBase, errorNew = detection(indis)\n",
    "#     aurocBase, aurocNew = auroc(indis)\n",
    "#     auprinBase, auprinNew = auprIn(indis)\n",
    "#     auproutBase, auproutNew = auprOut(indis)\n",
    "#     print(\"{:31}{:>22}\".format(\"Neural network architecture:\", nn))\n",
    "#     print(\"{:31}{:>22}\".format(\"In-distribution dataset:\", indis))\n",
    "#     print(\"{:31}{:>22}\".format(\"Out-of-distribution dataset:\", \"IMAGENET-O\"))\n",
    "#     print(\"\")\n",
    "#     print(\"{:>34}{:>19}\".format(\"Baseline\", \"Our Method\"))\n",
    "#     print(\"{:20}{:13.1f}%{:>18.1f}% \".format(\"FPR at TPR 95%:\",fprBase*100, fprNew*100))\n",
    "#     print(\"{:20}{:13.1f}%{:>18.1f}%\".format(\"Detection error:\",errorBase*100, errorNew*100))\n",
    "#     print(\"{:20}{:13.1f}%{:>18.1f}%\".format(\"AUROC:\",aurocBase*100, aurocNew*100))\n",
    "#     print(\"{:20}{:13.1f}%{:>18.1f}%\".format(\"AUPR In:\",auprinBase*100, auprinNew*100))\n",
    "#     print(\"{:20}{:13.1f}%{:>18.1f}%\".format(\"AUPR Out:\",auproutBase*100, auproutNew*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as sk\n",
    "recall_level_default = 0.95\n",
    "\n",
    "def stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"Use high precision for cumsum and check that final value matches sum\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        To be cumulatively summed as flat\n",
    "    rtol : float\n",
    "        Relative tolerance, see ``np.allclose``\n",
    "    atol : float\n",
    "        Absolute tolerance, see ``np.allclose``\n",
    "\n",
    "    Credits: https://github.com/wetliu/energy_ood/blob/7d31247d980b5337c519a61402d35141f5ec6289/utils/display_results.py#L7\n",
    "    \"\"\"\n",
    "    out = np.cumsum(arr, dtype=np.float64)\n",
    "    expected = np.sum(arr, dtype=np.float64)\n",
    "    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n",
    "        raise RuntimeError('cumsum was found to be unstable: '\n",
    "                           'its last element does not correspond to sum')\n",
    "    return out\n",
    "\n",
    "def calib_err(confidence, correct, p='2', beta=100):\n",
    "    # beta is target bin size\n",
    "    idxs = np.argsort(confidence)\n",
    "    confidence = confidence[idxs]\n",
    "    correct = correct[idxs]\n",
    "    bins = [[i * beta, (i + 1) * beta] for i in range(len(confidence) // beta)]\n",
    "    bins[-1] = [bins[-1][0], len(confidence)]\n",
    "\n",
    "    cerr = 0\n",
    "    total_examples = len(confidence)\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_confidence = confidence[bins[i][0]:bins[i][1]]\n",
    "        bin_correct = correct[bins[i][0]:bins[i][1]]\n",
    "        num_examples_in_bin = len(bin_confidence)\n",
    "\n",
    "        if num_examples_in_bin > 0:\n",
    "            difference = np.abs(np.nanmean(bin_confidence) - np.nanmean(bin_correct))\n",
    "\n",
    "            if p == '2':\n",
    "                cerr += num_examples_in_bin / total_examples * np.square(difference)\n",
    "            elif p == '1':\n",
    "                cerr += num_examples_in_bin / total_examples * difference\n",
    "            elif p == 'infty' or p == 'infinity' or p == 'max':\n",
    "                cerr = np.maximum(cerr, difference)\n",
    "            else:\n",
    "                assert False, \"p must be '1', '2', or 'infty'\"\n",
    "\n",
    "    if p == '2':\n",
    "        cerr = np.sqrt(cerr)\n",
    "\n",
    "    return cerr\n",
    "\n",
    "\n",
    "def aurra(confidence, correct):\n",
    "    conf_ranks = np.argsort(confidence)[::-1]  # indices from greatest to least confidence\n",
    "    rra_curve = np.cumsum(np.asarray(correct)[conf_ranks])\n",
    "    rra_curve = rra_curve / np.arange(1, len(rra_curve) + 1)  # accuracy at each response rate\n",
    "    return np.mean(rra_curve)\n",
    "\n",
    "\n",
    "def soft_f1(confidence, correct):\n",
    "    wrong = 1 - correct\n",
    "\n",
    "    # # the incorrectly classified samples are our interest\n",
    "    # # so they make the positive class\n",
    "    # tp_soft = np.sum((1 - confidence) * wrong)\n",
    "    # fp_soft = np.sum((1 - confidence) * correct)\n",
    "    # fn_soft = np.sum(confidence * wrong)\n",
    "\n",
    "    # return 2 * tp_soft / (2 * tp_soft + fn_soft + fp_soft)\n",
    "    return 2 * ((1 - confidence) * wrong).sum()/(1 - confidence + wrong).sum()\n",
    "\n",
    "\n",
    "def tune_temp(logits, labels, binary_search=True, lower=0.2, upper=5.0, eps=0.0001):\n",
    "    logits = np.array(logits)\n",
    "\n",
    "    if binary_search:\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        logits = torch.FloatTensor(logits)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        t_guess = torch.FloatTensor([0.5*(lower + upper)]).requires_grad_()\n",
    "\n",
    "        while upper - lower > eps:\n",
    "            if torch.autograd.grad(F.cross_entropy(logits / t_guess, labels), t_guess)[0] > 0:\n",
    "                upper = 0.5 * (lower + upper)\n",
    "            else:\n",
    "                lower = 0.5 * (lower + upper)\n",
    "            t_guess = t_guess * 0 + 0.5 * (lower + upper)\n",
    "\n",
    "        t = min([lower, 0.5 * (lower + upper), upper], key=lambda x: float(F.cross_entropy(logits / x, labels)))\n",
    "    else:\n",
    "        import cvxpy as cx\n",
    "\n",
    "        set_size = np.array(logits).shape[0]\n",
    "\n",
    "        t = cx.Variable()\n",
    "\n",
    "        expr = sum((cx.Minimize(cx.log_sum_exp(logits[i, :] * t) - logits[i, labels[i]] * t)\n",
    "                    for i in range(set_size)))\n",
    "        p = cx.Problem(expr, [lower <= t, t <= upper])\n",
    "\n",
    "        p.solve()   # p.solve(solver=cx.SCS)\n",
    "        t = 1 / t.value\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def print_measures(rms, aurra_metric, mad, sf1, method_name='Baseline'):\n",
    "    print('\\t\\t\\t\\t\\t\\t\\t' + method_name)\n",
    "    print('RMS Calib Error (%): \\t\\t{:.2f}'.format(100 * rms))\n",
    "    print('AURRA (%): \\t\\t\\t{:.2f}'.format(100 * aurra))\n",
    "    # print('MAD Calib Error (%): \\t\\t{:.2f}'.format(100 * mad))\n",
    "    # print('Soft F1 Score (%):   \\t\\t{:.2f}'.format(100 * sf1))\n",
    "\n",
    "\n",
    "def show_calibration_results(confidence, correct, method_name='Baseline'):\n",
    "\n",
    "    print('\\t\\t\\t\\t' + method_name)\n",
    "    print('RMS Calib Error (%): \\t\\t{:.2f}'.format(\n",
    "        100 * calib_err(confidence, correct, p='2')))\n",
    "\n",
    "    print('AURRA (%): \\t\\t\\t{:.2f}'.format(\n",
    "        100 * aurra(confidence, correct)))\n",
    "\n",
    "    # print('MAD Calib Error (%): \\t\\t{:.2f}'.format(\n",
    "    #     100 * calib_err(confidence, correct, p='1')))\n",
    "\n",
    "    # print('Soft F1-Score (%): \\t\\t{:.2f}'.format(\n",
    "    #     100 * soft_f1(confidence, correct)))\n",
    "\n",
    "def fpr_and_fdr_at_recall(y_true, y_score, recall_level=recall_level_default, pos_label=None):\n",
    "    classes = np.unique(y_true)\n",
    "    if (pos_label is None and\n",
    "            not (np.array_equal(classes, [0, 1]) or\n",
    "                     np.array_equal(classes, [-1, 1]) or\n",
    "                     np.array_equal(classes, [0]) or\n",
    "                     np.array_equal(classes, [-1]) or\n",
    "                     np.array_equal(classes, [1]))):\n",
    "        raise ValueError(\"Data is not binary and pos_label is not specified\")\n",
    "    elif pos_label is None:\n",
    "        pos_label = 1.\n",
    "\n",
    "    # make y_true a boolean vector\n",
    "    y_true = (y_true == pos_label)\n",
    "\n",
    "    # sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "\n",
    "    # y_score typically has many tied values. Here we extract\n",
    "    # the indices associated with the distinct values. We also\n",
    "    # concatenate a value for the end of the curve.\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "\n",
    "    # accumulate the true positives with decreasing threshold\n",
    "    tps = stable_cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps      # add one because of zero-based indexing\n",
    "\n",
    "    thresholds = y_score[threshold_idxs]\n",
    "\n",
    "    recall = tps / tps[-1]\n",
    "\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)      # [last_ind::-1]\n",
    "    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n",
    "\n",
    "    cutoff = np.argmin(np.abs(recall - recall_level))\n",
    "\n",
    "    return fps[cutoff] / (np.sum(np.logical_not(y_true)))   # , fps[cutoff]/(fps[cutoff] + tps[cutoff])\n",
    "\n",
    "def get_measures(_pos, _neg, recall_level=recall_level_default):\n",
    "    pos = np.array(_pos[:]).reshape((-1, 1))\n",
    "    neg = np.array(_neg[:]).reshape((-1, 1))\n",
    "    examples = np.squeeze(np.vstack((pos, neg)))\n",
    "    labels = np.zeros(len(examples), dtype=np.int32)\n",
    "    labels[:len(pos)] += 1\n",
    "\n",
    "    auroc = sk.roc_auc_score(labels, examples)\n",
    "    aupr = sk.average_precision_score(labels, examples)\n",
    "    fpr = fpr_and_fdr_at_recall(labels, examples, recall_level)\n",
    "\n",
    "    return auroc, aupr, fpr\n",
    "\n",
    "\n",
    "def print_measures_old(auroc, aupr, fpr, method_name='Ours', recall_level=recall_level_default):\n",
    "    print('\\t\\t\\t' + method_name)\n",
    "    print('FPR{:d}:\\t{:.2f}'.format(int(100 * recall_level), 100 * fpr))\n",
    "    print('AUROC: \\t{:.2f}'.format(100 * auroc))\n",
    "    print('AUPR:  \\t{:.2f}'.format(100 * aupr))\n",
    "\n",
    "\n",
    "def print_measures_with_std(aurocs, auprs, fprs, method_name='Ours', recall_level=recall_level_default):\n",
    "    print('\\t\\t\\t' + method_name)\n",
    "    print('FPR{:d}:\\t{:.2f}\\t+/- {:.2f}'.format(int(100 * recall_level), 100 * np.mean(fprs), 100 * np.std(fprs)))\n",
    "    print('AUROC: \\t{:.2f}\\t+/- {:.2f}'.format(100 * np.mean(aurocs), 100 * np.std(aurocs)))\n",
    "    print('AUPR:  \\t{:.2f}\\t+/- {:.2f}'.format(100 * np.mean(auprs), 100 * np.std(auprs)))\n",
    "\n",
    "\n",
    "def get_and_print_results(out_score, in_score, num_to_avg=1):\n",
    "\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    #for _ in range(num_to_avg):\n",
    "    #    out_score = get_ood_scores(ood_loader)\n",
    "    measures = get_measures(out_score, in_score)\n",
    "    aurocs.append(measures[0]); auprs.append(measures[1]); fprs.append(measures[2])\n",
    "\n",
    "    auroc = np.mean(aurocs); aupr = np.mean(auprs); fpr = np.mean(fprs)\n",
    "    #auroc_list.append(auroc); aupr_list.append(aupr); fpr_list.append(fpr)\n",
    "\n",
    "    #if num_to_avg >= 5:\n",
    "    #    print_measures_with_std(aurocs, auprs, fprs, method_name='Ours')\n",
    "    #else:\n",
    "    #    print_measures(auroc, aupr, fpr, method_name='Ours')\n",
    "    return auroc, aupr, fpr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RZFJlH04dDBu"
   },
   "outputs": [],
   "source": [
    "to_np = lambda x: x.data.to('cpu').numpy()\n",
    "\n",
    "def testData(net, criterion, CUDA_DEVICE, testloader10, testloader, noiseMagnitude1, temper):\n",
    "    t0 = time.time()\n",
    "    f1 = open(PATH_CONFIDENCE_BASE_IN, 'w+')\n",
    "    f2 = open(PATH_CONFIDENCE_BASE_OUT, 'w+')\n",
    "    g1 = open(PATH_CONFIDENCE_OUR_IN, 'w+')\n",
    "    g2 = open(PATH_CONFIDENCE_OUR_OUT, 'w+')\n",
    "    N = 10000\n",
    "    print(\"Processing in-distribution images\")\n",
    "########################################In-distribution###########################################\n",
    "    for j, data in enumerate(testloader10):\n",
    "#         if j<1000: continue\n",
    "        images, _ = data\n",
    "        \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here, no temperature scaling used\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "#         print(temper, noiseMagnitude1, np.max(nnOutputs))\n",
    "        f1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "\t\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  torch.ge(inputs.grad.data, 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image  # std: [0.229, 0.224, 0.225]\n",
    "        gradient[0][0] = (gradient[0][0] )/0.229\n",
    "        gradient[0][1] = (gradient[0][1] )/0.224\n",
    "        gradient[0][2] = (gradient[0][2])/0.225\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g1.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "#             print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "        \n",
    "        if j == N - 1: break\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    print(\"Processing out-of-distribution images\")\n",
    "###################################Out-of-Distributions#####################################\n",
    "    for j, data in enumerate(testloader):\n",
    "#         if j<1000: continue\n",
    "        images, _ = data\n",
    "    \n",
    "        inputs = Variable(images.cuda(CUDA_DEVICE), requires_grad = True)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "\n",
    "        # Calculating the confidence of the output, no perturbation added here\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "#         print(temper, noiseMagnitude1, np.max(nnOutputs))\n",
    "        f2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        \n",
    "        # Using temperature scaling\n",
    "        outputs = outputs / temper\n",
    "  \n",
    "  \n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        maxIndexTemp = np.argmax(nnOutputs)\n",
    "        labels = Variable(torch.LongTensor([maxIndexTemp]).cuda(CUDA_DEVICE))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient =  (torch.ge(inputs.grad.data, 0))\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "        # Normalizing the gradient to the same space of image # std: [0.229, 0.224, 0.225]\n",
    "        gradient[0][0] = (gradient[0][0] )/0.229\n",
    "        gradient[0][1] = (gradient[0][1] )/0.224\n",
    "        gradient[0][2] = (gradient[0][2])/0.225\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n",
    "        outputs = net(Variable(tempInputs))\n",
    "        outputs = outputs / temper\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutputs = outputs.data.cpu()\n",
    "        nnOutputs = nnOutputs.numpy()\n",
    "        nnOutputs = nnOutputs[0]\n",
    "        nnOutputs = nnOutputs - np.max(nnOutputs)\n",
    "        nnOutputs = np.exp(nnOutputs)/np.sum(np.exp(nnOutputs))\n",
    "        g2.write(\"{}, {}, {}\\n\".format(temper, noiseMagnitude1, np.max(nnOutputs)))\n",
    "        if j % 100 == 99:\n",
    "#             print(\"{:4}/{:4} images processed, {:.1f} seconds used.\".format(j+1-1000, N-1000, time.time()-t0))\n",
    "            t0 = time.time()\n",
    "\n",
    "        if j== N-1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b14m_1f_XZPb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet \n",
      "\n",
      "Processing in-distribution images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-12686078/ipykernel_1478858/1984889901.py:47: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  tempInputs = torch.add(inputs.data,  -noiseMagnitude1, gradient)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing out-of-distribution images\n",
      "\t\t\tBaseline\n",
      "FPR95:\t88.01\n",
      "AUROC: \t50.51\n",
      "AUPR:  \t15.33\n",
      "\t\t\tODIN\n",
      "FPR95:\t89.14\n",
      "AUROC: \t52.07\n",
      "AUPR:  \t15.96\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "squeezenet1_1 \n",
      "\n",
      "Processing in-distribution images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing out-of-distribution images\n",
      "\t\t\tBaseline\n",
      "FPR95:\t89.18\n",
      "AUROC: \t50.86\n",
      "AUPR:  \t15.53\n",
      "\t\t\tODIN\n",
      "FPR95:\t91.69\n",
      "AUROC: \t51.55\n",
      "AUPR:  \t16.15\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 \n",
      "\n",
      "Processing in-distribution images\n",
      "Processing out-of-distribution images\n",
      "\t\t\tBaseline\n",
      "FPR95:\t89.28\n",
      "AUROC: \t46.67\n",
      "AUPR:  \t14.18\n",
      "\t\t\tODIN\n",
      "FPR95:\t90.48\n",
      "AUROC: \t43.63\n",
      "AUPR:  \t13.47\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg19 \n",
      "\n",
      "Processing in-distribution images\n",
      "Processing out-of-distribution images\n",
      "\t\t\tBaseline\n",
      "FPR95:\t90.44\n",
      "AUROC: \t46.28\n",
      "AUPR:  \t14.08\n",
      "\t\t\tODIN\n",
      "FPR95:\t91.12\n",
      "AUROC: \t43.00\n",
      "AUPR:  \t13.34\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg19_bn \n",
      "\n",
      "Processing in-distribution images\n",
      "Processing out-of-distribution images\n",
      "\t\t\tBaseline\n",
      "FPR95:\t87.17\n",
      "AUROC: \t46.88\n",
      "AUPR:  \t14.19\n",
      "\t\t\tODIN\n",
      "FPR95:\t90.64\n",
      "AUROC: \t42.71\n",
      "AUPR:  \t13.28\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scratch/as14770/CV-FinalProject/.cvfpmodels/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet121 \n",
      "\n",
      "Processing in-distribution images\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_IMAGENET_O = \"/scratch/as14770/CV-FinalProject/DATA/imagenet-o\"\n",
    "PATH_TO_IMAGENET_VAL = \"/scratch/as14770/CV-FinalProject/DATA/imagenet1k-val\"\n",
    "TORCH_HOME_DIR = \"/scratch/as14770/CV-FinalProject/.cvfpmodels/\"\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "test_transform = trn.Compose(\n",
    "    [trn.Resize(256), trn.CenterCrop(224), trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "noes = dset.ImageFolder(root=PATH_TO_IMAGENET_O, transform=test_transform)\n",
    "noe_loader = torch.utils.data.DataLoader(noes, batch_size=1, shuffle=False,\n",
    "                                         num_workers=4, pin_memory=True)\n",
    "\n",
    "imagenet_o_folder = \"/scratch/as14770/CV-FinalProject/imagenet-o-symlink/\"\n",
    "\n",
    "def create_symlinks_to_imagenet(imagenet_folder, folder_to_scan):\n",
    "    if not os.path.exists(imagenet_folder):\n",
    "        os.makedirs(imagenet_folder)\n",
    "        folders_of_interest = os.listdir(folder_to_scan)\n",
    "        path_prefix = PATH_TO_IMAGENET_VAL \n",
    "        for folder in folders_of_interest:\n",
    "            os.symlink(path_prefix + folder, imagenet_folder+folder, target_is_directory=True)\n",
    "\n",
    "create_symlinks_to_imagenet(imagenet_o_folder, PATH_TO_IMAGENET_O)\n",
    "\n",
    "val_examples_imagenet_o = dset.ImageFolder(root=imagenet_o_folder, transform=test_transform)\n",
    "val_loader_imagenet_o = torch.utils.data.DataLoader(val_examples_imagenet_o, batch_size=1, shuffle=False,\n",
    "                                         num_workers=4, pin_memory=True)\n",
    "\n",
    "val_imagenet = dset.ImageNet(root=PATH_TO_IMAGENET_VAL, split = 'val', transform=test_transform)\n",
    "val_imagenet_loader = torch.utils.data.DataLoader(val_imagenet, batch_size=1, shuffle=False,\n",
    "                                         num_workers=4, pin_memory=True)\n",
    "\n",
    "concat = lambda x: np.concatenate(x, axis=0)\n",
    "to_np = lambda x: x.data.to('cpu').numpy()\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = TORCH_HOME_DIR\n",
    "#print(os.environ[\"TORCH_HOME\"])\n",
    "torch.hub.set_dir(TORCH_HOME_DIR)\n",
    "models_to_test = [\n",
    "    (\"pytorch/vision\", \"alexnet\"),\n",
    "    (\"pytorch/vision\", \"squeezenet1_1\"),\n",
    "    (\"pytorch/vision\", \"vgg16\"),\n",
    "    (\"pytorch/vision\", \"vgg19\"),\n",
    "    ('pytorch/vision', \"vgg19_bn\"),\n",
    "    ('pytorch/vision', \"densenet121\"),\n",
    "    ('pytorch/vision', \"resnet50\"),\n",
    "    ('pytorch/vision', \"resnet101\"),\n",
    "    ('pytorch/vision', \"resnet152\"),\n",
    "    ('pytorch/vision', \"wide_resnet50_2\"),\n",
    "    ('pytorch/vision', \"resnext101_32x8d\"),\n",
    "    ('pytorch/vision', \"resnext50_32x4d\"),\n",
    "    ('facebookresearch/WSL-Images', \"resnext101_32x8d_wsl\"),\n",
    "    ('facebookresearch/WSL-Images', \"resnext101_32x16d_wsl\"),\n",
    "    ('facebookresearch/WSL-Images', \"resnext101_32x32d_wsl\"),\n",
    "    (\"pretrained\", \"dpn68\"),\n",
    "    (\"pretrained\", \"dpn98\"),\n",
    "    (\"pretrained\", \"se_resnet101\"),\n",
    "    (\"pretrained\", \"se_resnet152\"),\n",
    "    (\"pretrained\", \"resnext101_32x4d\"),\n",
    "    (\"pretrained\", \"se_resnext101_32x4d\"),\n",
    "    (\"facebookresearch/deit:main\", \"deit_base_patch16_224\"),\n",
    "    (\"facebookresearch/deit:main\", \"deit_small_patch16_224\"),\n",
    "    (\"facebookresearch/deit:main\", \"deit_tiny_patch16_224\"),\n",
    "]\n",
    "\n",
    "for net_params in models_to_test:\n",
    "    if net_params[0] == \"pytorch/vision\":\n",
    "        net = torch.hub.load('pytorch/vision:v0.10.0', net_params[1], pretrained=True)\n",
    "#         net = models.alexnet(pretrained=True)\n",
    "    elif \"facebookresearch/deit\" in net_params[0]:\n",
    "        net = torch.hub.load(net_params[0], net_params[1], pretrained=True)\n",
    "    elif net_params[0] == \"pretrained\":\n",
    "        net = pretrainedmodels.__dict__[net_params[1]](num_classes=1000, pretrained='imagenet')\n",
    "    else:\n",
    "        net = torch.hub.load('/scratch/as14770/CV-FinalProject/facebookresearch_WSL-Images_master', net_params[1], source='local')\n",
    "    print(net_params[1], '\\n')\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "    testData(net, criterion, CUDA_DEVICE, val_loader_imagenet_o, noe_loader, epsilon, temperature) \n",
    "    # Get Baseline Results\n",
    "    indistr = np.loadtxt(PATH_CONFIDENCE_BASE_IN, delimiter=',')\n",
    "    outdistr = np.loadtxt(PATH_CONFIDENCE_BASE_OUT, delimiter=',')\n",
    "#     start = 0.001\n",
    "#     end = 1    \n",
    "#     gap = (end- start)/100000\n",
    "    Y1 = outdistr[:, 2]\n",
    "#     Y1 = -Y1\n",
    "    X1 = indistr[:, 2]\n",
    "#     X1 = -X1\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    measures = get_measures(Y1, X1)\n",
    "    aurocs = measures[0]; auprs = measures[1]; fprs = measures[2]\n",
    "    print_measures_old(aurocs, auprs, fprs, method_name='Baseline')\n",
    "    # Get ODIN Results\n",
    "    indistr = np.loadtxt(PATH_CONFIDENCE_OUR_IN, delimiter=',')\n",
    "    outdistr = np.loadtxt(PATH_CONFIDENCE_OUR_OUT, delimiter=',')\n",
    "#     start = 0.001 #To be tuned\n",
    "#     end = 0.0011 # To be tuned\n",
    "#     gap = (end- start)/100000\n",
    "    Y1 = outdistr[:, 2]\n",
    "#     Y1 = -Y1\n",
    "    X1 = indistr[:, 2]\n",
    "#     X1 = -X1\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    measures = get_measures(Y1, X1)\n",
    "    aurocs = measures[0]; auprs = measures[1]; fprs = measures[2]\n",
    "    print_measures_old(aurocs, auprs, fprs, method_name='ODIN')\n",
    "#     metric(net_params[1])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CV_Project_ODIN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
